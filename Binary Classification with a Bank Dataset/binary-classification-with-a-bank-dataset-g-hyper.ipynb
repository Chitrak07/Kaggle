{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91719,"databundleVersionId":12937777,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import GridSearchCV\n\n# --- 0. Load the Data ---\ntry:\n    train_df = pd.read_csv('/kaggle/input/playground-series-s5e8/train.csv')\n    test_df = pd.read_csv('/kaggle/input/playground-series-s5e8/test.csv')\n    print(\"Files loaded successfully!\")\nexcept FileNotFoundError as e:\n    print(f\"File not found. Please ensure the data files are in the correct directory. Error: {e}\")\n    exit()\n\n# --- 1. Data Preparation (Same as before) ---\nX = train_df.drop('y', axis=1)\ny = train_df['y']\ntest_ids = test_df['id']\ncombined_df = pd.concat([X, test_df], ignore_index=True)\ncategorical_features = combined_df.select_dtypes(include=['object']).columns\ncombined_df = pd.get_dummies(combined_df, columns=categorical_features, drop_first=True)\nX_processed = combined_df.iloc[:len(train_df)].drop('id', axis=1)\nX_test_processed = combined_df.iloc[len(train_df):].drop('id', axis=1)\n\n# --- 2. Hyperparameter Tuning Setup ---\n\n# Handle class imbalance for LightGBM\nscale_pos_weight = y.value_counts()[0] / y.value_counts()[1]\n\n# Initialize the LightGBM model\nlgbm = lgb.LGBMClassifier(objective='binary',\n                          random_state=42,\n                          n_jobs=-1,\n                          scale_pos_weight=scale_pos_weight)\n\n# Define a smaller grid of parameters to search.\n# A larger grid would give better results but take much longer.\nparam_grid = {\n    'n_estimators': [400, 600],\n    'learning_rate': [0.05, 0.1],\n    'num_leaves': [31, 40],\n}\n\n# Set up the Grid Search with 3-fold cross-validation\n# The model will be evaluated using the ROC AUC score\ngrid_search = GridSearchCV(estimator=lgbm,\n                           param_grid=param_grid,\n                           scoring='roc_auc',\n                           cv=3,       # 3-fold cross-validation\n                           verbose=2)  # Shows progress\n\n# --- 3. Model Training ---\nprint(\"Starting Hyperparameter Tuning with GridSearchCV... This will take a significant amount of time.\")\ngrid_search.fit(X_processed, y)\nprint(\"Tuning complete.\")\n\n# Print the best parameters found\nprint(\"\\nBest parameters found by Grid Search:\")\nprint(grid_search.best_params_)\nprint(f\"Best ROC AUC score on validation data: {grid_search.best_score_:.5f}\")\n\n\n# --- 4. Prediction and Submission ---\nprint(\"\\nMaking predictions using the best model...\")\n# The grid_search object automatically retains the best model found\nbest_model = grid_search.best_estimator_\ntest_probabilities = best_model.predict_proba(X_test_processed)[:, 1]\n\n# Create and save the new submission file\nsubmission_df = pd.DataFrame({'id': test_ids, 'y': test_probabilities})\nsubmission_df.to_csv('submission_tuned.csv', index=False)\n\nprint(\"\\nNew submission file 'submission_tuned.csv' created successfully!\")\nprint(submission_df.head())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-09T08:17:47.458679Z","iopub.execute_input":"2025-08-09T08:17:47.458919Z","iopub.status.idle":"2025-08-09T08:24:16.438292Z","shell.execute_reply.started":"2025-08-09T08:17:47.458894Z","shell.execute_reply":"2025-08-09T08:24:16.437504Z"}},"outputs":[{"name":"stdout","text":"Files loaded successfully!\nStarting Hyperparameter Tuning with GridSearchCV... This will take a significant amount of time.\nFitting 3 folds for each of 8 candidates, totalling 24 fits\n[LightGBM] [Info] Number of positive: 60326, number of negative: 439674\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035182 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1023\n[LightGBM] [Info] Number of data points in the train set: 500000, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986270\n[LightGBM] [Info] Start training from score -1.986270\n[CV] END learning_rate=0.05, n_estimators=400, num_leaves=31; total time=  12.0s\n[LightGBM] [Info] Number of positive: 60325, number of negative: 439675\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032056 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1025\n[LightGBM] [Info] Number of data points in the train set: 500000, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END learning_rate=0.05, n_estimators=400, num_leaves=31; total time=  12.2s\n[LightGBM] [Info] Number of positive: 60325, number of negative: 439675\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032188 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1024\n[LightGBM] [Info] Number of data points in the train set: 500000, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END learning_rate=0.05, n_estimators=400, num_leaves=31; total time=  13.8s\n[LightGBM] [Info] Number of positive: 60326, number of negative: 439674\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033171 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1023\n[LightGBM] [Info] Number of data points in the train set: 500000, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986270\n[LightGBM] [Info] Start training from score -1.986270\n[CV] END learning_rate=0.05, n_estimators=400, num_leaves=40; total time=  12.7s\n[LightGBM] [Info] Number of positive: 60325, number of negative: 439675\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033791 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1025\n[LightGBM] [Info] Number of data points in the train set: 500000, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END learning_rate=0.05, n_estimators=400, num_leaves=40; total time=  15.6s\n[LightGBM] [Info] Number of positive: 60325, number of negative: 439675\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032378 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1024\n[LightGBM] [Info] Number of data points in the train set: 500000, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END learning_rate=0.05, n_estimators=400, num_leaves=40; total time=  12.9s\n[LightGBM] [Info] Number of positive: 60326, number of negative: 439674\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032343 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1023\n[LightGBM] [Info] Number of data points in the train set: 500000, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986270\n[LightGBM] [Info] Start training from score -1.986270\n[CV] END learning_rate=0.05, n_estimators=600, num_leaves=31; total time=  16.3s\n[LightGBM] [Info] Number of positive: 60325, number of negative: 439675\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.128887 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1025\n[LightGBM] [Info] Number of data points in the train set: 500000, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END learning_rate=0.05, n_estimators=600, num_leaves=31; total time=  16.9s\n[LightGBM] [Info] Number of positive: 60325, number of negative: 439675\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032701 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1024\n[LightGBM] [Info] Number of data points in the train set: 500000, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END learning_rate=0.05, n_estimators=600, num_leaves=31; total time=  16.7s\n[LightGBM] [Info] Number of positive: 60326, number of negative: 439674\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033154 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1023\n[LightGBM] [Info] Number of data points in the train set: 500000, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986270\n[LightGBM] [Info] Start training from score -1.986270\n[CV] END learning_rate=0.05, n_estimators=600, num_leaves=40; total time=  19.1s\n[LightGBM] [Info] Number of positive: 60325, number of negative: 439675\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.131790 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1025\n[LightGBM] [Info] Number of data points in the train set: 500000, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END learning_rate=0.05, n_estimators=600, num_leaves=40; total time=  21.1s\n[LightGBM] [Info] Number of positive: 60325, number of negative: 439675\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032377 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1024\n[LightGBM] [Info] Number of data points in the train set: 500000, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END learning_rate=0.05, n_estimators=600, num_leaves=40; total time=  17.6s\n[LightGBM] [Info] Number of positive: 60326, number of negative: 439674\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032465 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1023\n[LightGBM] [Info] Number of data points in the train set: 500000, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986270\n[LightGBM] [Info] Start training from score -1.986270\n[CV] END .learning_rate=0.1, n_estimators=400, num_leaves=31; total time=  12.2s\n[LightGBM] [Info] Number of positive: 60325, number of negative: 439675\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033597 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1025\n[LightGBM] [Info] Number of data points in the train set: 500000, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END .learning_rate=0.1, n_estimators=400, num_leaves=31; total time=  10.3s\n[LightGBM] [Info] Number of positive: 60325, number of negative: 439675\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032446 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1024\n[LightGBM] [Info] Number of data points in the train set: 500000, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END .learning_rate=0.1, n_estimators=400, num_leaves=31; total time=  10.5s\n[LightGBM] [Info] Number of positive: 60326, number of negative: 439674\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031831 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1023\n[LightGBM] [Info] Number of data points in the train set: 500000, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986270\n[LightGBM] [Info] Start training from score -1.986270\n[CV] END .learning_rate=0.1, n_estimators=400, num_leaves=40; total time=  12.9s\n[LightGBM] [Info] Number of positive: 60325, number of negative: 439675\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032589 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1025\n[LightGBM] [Info] Number of data points in the train set: 500000, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END .learning_rate=0.1, n_estimators=400, num_leaves=40; total time=  11.0s\n[LightGBM] [Info] Number of positive: 60325, number of negative: 439675\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032094 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1024\n[LightGBM] [Info] Number of data points in the train set: 500000, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END .learning_rate=0.1, n_estimators=400, num_leaves=40; total time=  11.1s\n[LightGBM] [Info] Number of positive: 60326, number of negative: 439674\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032187 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1023\n[LightGBM] [Info] Number of data points in the train set: 500000, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986270\n[LightGBM] [Info] Start training from score -1.986270\n[CV] END .learning_rate=0.1, n_estimators=600, num_leaves=31; total time=  16.8s\n[LightGBM] [Info] Number of positive: 60325, number of negative: 439675\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032594 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1025\n[LightGBM] [Info] Number of data points in the train set: 500000, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END .learning_rate=0.1, n_estimators=600, num_leaves=31; total time=  14.3s\n[LightGBM] [Info] Number of positive: 60325, number of negative: 439675\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032311 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1024\n[LightGBM] [Info] Number of data points in the train set: 500000, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END .learning_rate=0.1, n_estimators=600, num_leaves=31; total time=  16.5s\n[LightGBM] [Info] Number of positive: 60326, number of negative: 439674\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031906 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1023\n[LightGBM] [Info] Number of data points in the train set: 500000, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986270\n[LightGBM] [Info] Start training from score -1.986270\n[CV] END .learning_rate=0.1, n_estimators=600, num_leaves=40; total time=  15.5s\n[LightGBM] [Info] Number of positive: 60325, number of negative: 439675\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033010 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1025\n[LightGBM] [Info] Number of data points in the train set: 500000, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END .learning_rate=0.1, n_estimators=600, num_leaves=40; total time=  17.2s\n[LightGBM] [Info] Number of positive: 60325, number of negative: 439675\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032677 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1024\n[LightGBM] [Info] Number of data points in the train set: 500000, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END .learning_rate=0.1, n_estimators=600, num_leaves=40; total time=  15.5s\n[LightGBM] [Info] Number of positive: 90488, number of negative: 659512\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048061 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1027\n[LightGBM] [Info] Number of data points in the train set: 750000, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120651 -> initscore=-1.986283\n[LightGBM] [Info] Start training from score -1.986283\nTuning complete.\n\nBest parameters found by Grid Search:\n{'learning_rate': 0.1, 'n_estimators': 600, 'num_leaves': 40}\nBest ROC AUC score on validation data: 0.96691\n\nMaking predictions using the best model...\n\nNew submission file 'submission_tuned.csv' created successfully!\n       id         y\n0  750000  0.008331\n1  750001  0.540759\n2  750002  0.000731\n3  750003  0.000032\n4  750004  0.124760\n","output_type":"stream"}],"execution_count":1}]}